{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHdsXUM8tQuO",
        "outputId": "62c63b14-bbcc-4b49-dd7e-224689e35f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Nov  8 18:27:53 2020\n",
        "\n",
        "@author: Livnat\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from math import ceil\n",
        "from random import Random\n",
        "\n",
        "\"\"\" Dataset partitioning helper \"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\t\n",
        "\n",
        "\n",
        "class Partition(object):\n",
        "\n",
        "    def __init__(self, data, index):\n",
        "        self.data = data\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_idx = self.index[index]\n",
        "        return self.data[data_idx]\n",
        "\n",
        "\n",
        "class DataPartitioner(object):\n",
        "\n",
        "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
        "        self.data = data\n",
        "        self.partitions = []\n",
        "        rng = Random()\n",
        "        rng.seed(seed)\n",
        "        data_len = len(data)\n",
        "        indexes = [x for x in range(0, data_len)]\n",
        "        rng.shuffle(indexes)\n",
        "\n",
        "        for frac in sizes:\n",
        "            part_len = int(frac * data_len)\n",
        "            self.partitions.append(indexes[0:part_len])\n",
        "            indexes = indexes[part_len:]\n",
        "\n",
        "    def use(self, partition):\n",
        "        return Partition(self.data, self.partitions[partition])\n",
        "\t\n",
        "def partition_dataset():\n",
        "\t\t   dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "\t\t                             transform=transforms.Compose([\n",
        "\t\t                                 transforms.ToTensor(),\n",
        "\t\t                                 transforms.Normalize((0.1307,), (0.3081,))\n",
        "\t\t                             ]))\n",
        "\t\t   size = dist.get_world_size()\n",
        "\t\t   bsz = int(128 / (size))\n",
        "\t\t   partition_sizes = [1.0 / size for _ in range(size)]\n",
        "\t\t   partition = DataPartitioner(dataset, partition_sizes)\n",
        "\t\t   partition = partition.use(dist.get_rank())\n",
        "\t\t   train_set = torch.utils.data.DataLoader(partition,\n",
        "\t\t                                         batch_size=bsz,\n",
        "\t\t                                         shuffle=True)\n",
        "\t\t   return train_set, bsz\n",
        "\t\n",
        "\n",
        "def average_gradients(model):\n",
        "\t    size = float(dist.get_world_size())\n",
        "\t    for param in model.parameters():\n",
        "\t        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
        "\t        param.grad.data /= size\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "def run(rank, size):\n",
        "\t    torch.manual_seed(1234)\n",
        "\t    train_set, bsz = partition_dataset()\n",
        "\t    model = Net()\n",
        "\t    optimizer = optim.SGD(model.parameters(),\n",
        "\t                          lr=0.01, momentum=0.5)\n",
        "\t\n",
        "\t    num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
        "\t    for epoch in range(10):\n",
        "\t\t\t      epoch_loss = 0.0\n",
        "\t\t\t      correct = 0.0\n",
        "\t\t\t      for data, target in train_set:\n",
        "\t\t\t\t\t      optimizer.zero_grad()\n",
        "\t\t\t\t\t      output = model(data)\n",
        "\t\t\t\t\t      loss = F.nll_loss(output, target)\n",
        "\t\t\t\t\t      epoch_loss += loss.item()\n",
        "\t\t\t\t\t      loss.backward()\n",
        "\t\t\t\t\t      average_gradients(model)\n",
        "\t\t\t\t\t      optimizer.step()\n",
        "\t\t\t\t\t      pred = output.data.max(1, keepdim=True)[1]\n",
        "\t\t\t\t\t      correct += pred.eq(target.data.view_as(pred)).cpu().sum()\t\t\t      \n",
        "\t\t\t      print('Rank ', dist.get_rank(), ', epoch ',\n",
        "\t              epoch, ': ', epoch_loss / num_batches , 'Accuracy:',  100. * correct / num_batches)\n",
        "\t\t\t\n",
        "\n",
        "\n",
        "\t\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\t\n",
        "\n",
        "\t\n",
        "if __name__ == \"__main__\":\n",
        "    size = 2\n",
        "    processes = []\n",
        "    __spec__ = \"ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)\"\n",
        "    for rank in range(size):\n",
        "        p = Process(target=init_process, args=(rank, size, run))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:126: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:126: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
            "Process Process-25:\n",
            "Traceback (most recent call last):\n",
            "Process Process-26:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-25-43c62ca376a6>\", line 135, in init_process\n",
            "    fn(rank, size)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-25-43c62ca376a6>\", line 114, in run\n",
            "    for data, target in train_set:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-25-43c62ca376a6>\", line 135, in init_process\n",
            "    fn(rank, size)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"<ipython-input-25-43c62ca376a6>\", line 114, in run\n",
            "    for data, target in train_set:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-25-43c62ca376a6>\", line 57, in __getitem__\n",
            "    return self.data[data_idx]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\", line 103, in __getitem__\n",
            "    img = Image.fromarray(img.numpy(), mode='L')\n",
            "  File \"<ipython-input-25-43c62ca376a6>\", line 57, in __getitem__\n",
            "    return self.data[data_idx]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2701, in fromarray\n",
            "    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\", line 103, in __getitem__\n",
            "    img = Image.fromarray(img.numpy(), mode='L')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2636, in frombuffer\n",
            "    im = new(mode, (1, 1))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2544, in new\n",
            "    return im._new(core.fill(mode, size, color))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2701, in fromarray\n",
            "    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 527, in _new\n",
            "    new = Image()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2636, in frombuffer\n",
            "    im = new(mode, (1, 1))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-43c62ca376a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 506, in __init__\n",
            "    self._size = (0, 0)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2537, in new\n",
            "    im = Image()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 504, in __init__\n",
            "    self.im = None\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIQU4d4j0swe",
        "outputId": "4432c079-44ed-4f3c-cdfe-8b8a962ee8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Nov  8 18:27:53 2020\n",
        "\n",
        "@author: Livnat\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from math import ceil\n",
        "from random import Random\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\"\"\" Dataset partitioning helper \"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\t\n",
        "\n",
        "\n",
        "class Partition(object):\n",
        "\n",
        "    def __init__(self, data, index):\n",
        "        self.data = data\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_idx = self.index[index]\n",
        "        return self.data[data_idx]\n",
        "\n",
        "\n",
        "class DataPartitioner(object):\n",
        "\n",
        "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
        "        self.data = data\n",
        "        self.partitions = []\n",
        "        rng = Random()\n",
        "        rng.seed(seed)\n",
        "        data_len = len(data)\n",
        "        indexes = [x for x in range(0, data_len)]\n",
        "        rng.shuffle(indexes)\n",
        "\n",
        "        for frac in sizes:\n",
        "            part_len = int(frac * data_len)\n",
        "            self.partitions.append(indexes[0:part_len])\n",
        "            indexes = indexes[part_len:]\n",
        "\n",
        "    def use(self, partition):\n",
        "        return Partition(self.data, self.partitions[partition])\n",
        "\t\n",
        "def partition_dataset():\n",
        "\t\t   dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "\t\t                             transform=transforms.Compose([\n",
        "\t\t                                 transforms.ToTensor(),\n",
        "\t\t                                 transforms.Normalize((0.1307,), (0.3081,))\n",
        "\t\t                             ]))\n",
        "       \n",
        "\t\t   size = dist.get_world_size()\n",
        "\t\t   bsz = int(128 / (size))\n",
        "\t\t   partition_sizes = [1.0 / size for _ in range(size)]\n",
        "\t\t   partition = DataPartitioner(dataset, partition_sizes)\n",
        "\t\t   partition = partition.use(dist.get_rank())\n",
        "\n",
        "\t\t   indices = list(range(int(len(dataset)*partition_sizes[dist.get_rank()])))\n",
        "\t\t   np.random.shuffle(indices)\n",
        "\t\t   split = int(np.floor(0.1*len(dataset)))\n",
        "\t\t   train_sample = SubsetRandomSampler(indices[split:])\n",
        "\t\t   valid_sample = SubsetRandomSampler(indices[:split])\n",
        "\t\t   train_set = torch.utils.data.DataLoader(partition, sampler = train_sample,\n",
        "\t\t                                         batch_size=bsz )       \n",
        "\t\t   validation_set=torch.utils.data.DataLoader(partition, sampler = valid_sample, batch_size=bsz)  \n",
        "\t\t   return train_set,validation_set, bsz\n",
        "\t\n",
        "\n",
        "def average_gradients(model):\n",
        "\t    size = float(dist.get_world_size())\n",
        "\t    for param in model.parameters():\n",
        "\t        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
        "\t        param.grad.data /= size\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "def run(rank, size):\n",
        "\t    torch.manual_seed(1234)\n",
        "\t    train_set,validation_set, bsz = partition_dataset()\n",
        "\t    model = Net()\n",
        "\t    optimizer = optim.SGD(model.parameters(),\n",
        "\t                          lr=0.01, momentum=0.5)\n",
        "\t\n",
        "\t    train_num_batches = ceil(len(train_set.sampler) / float(bsz))\n",
        "\t    valid_num_batches = ceil(len(validation_set.sampler) / float(bsz))\n",
        "     \n",
        "\t    for epoch in range(10):\n",
        "\t\t\t      epoch_loss = 0.0\n",
        "\t\t\t      correct = 0.0\n",
        "\t\t\t      valid_loss = 0.0\n",
        "\t\t\t      correct_val = 0.0            \n",
        "\t\t\t      for data, target in train_set:\n",
        "\t\t\t\t\t      optimizer.zero_grad()\n",
        "\t\t\t\t\t      output = model(data)\n",
        "\t\t\t\t\t      loss = F.nll_loss(output, target)\n",
        "\t\t\t\t\t      epoch_loss += loss.item()\n",
        "\t\t\t\t\t      loss.backward()\n",
        "\t\t\t\t\t      average_gradients(model)\n",
        "\t\t\t\t\t      optimizer.step()\n",
        "\t\t\t\t\t      pred = output.data.max(1, keepdim=True)[1]\n",
        "\t\t\t\t\t      correct += pred.eq(target.data.view_as(pred)).cpu().sum()\t\n",
        "\t\t\t      for data, target in validation_set:\n",
        "\t\t\t\t\t      output = model(data)\n",
        "\t\t\t\t\t      loss = F.nll_loss(output, target)\n",
        "\t\t\t\t\t      valid_loss += loss.item()\n",
        "\t\t\t\t\t      pred = output.data.max(1, keepdim=True)[1]\n",
        "\t\t\t\t\t      correct_val += pred.eq(target.data.view_as(pred)).cpu().sum()\t\t\t      \n",
        "\t\t\t      print('Rank ', dist.get_rank(), ', epoch ',\n",
        "\t              epoch, ': ', epoch_loss / (train_num_batches) , 'Train Accuracy:',  100. * correct / (len(train_set.sampler)), 'Valid Loss: ', valid_loss / (valid_num_batches) , 'Valid Accuracy:',  100. * correct_val / (len(validation_set.sampler)))\n",
        "\t\t\t\n",
        "\n",
        "\n",
        "\t\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\t\n",
        "\n",
        "\t\n",
        "if __name__ == \"__main__\":\n",
        "    size = 2\n",
        "    processes = []\n",
        "    __spec__ = \"ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)\"\n",
        "    for rank in range(size):\n",
        "        p = Process(target=init_process, args=(rank, size, run))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:126: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:126: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
            "Process Process-67:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process Process-68:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-38-03122ed3e5b5>\", line 154, in init_process\n",
            "    fn(rank, size)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-38-03122ed3e5b5>\", line 127, in run\n",
            "    for data, target in train_set:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"<ipython-input-38-03122ed3e5b5>\", line 154, in init_process\n",
            "    fn(rank, size)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"<ipython-input-38-03122ed3e5b5>\", line 127, in run\n",
            "    for data, target in train_set:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 475, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"<ipython-input-38-03122ed3e5b5>\", line 59, in __getitem__\n",
            "    return self.data[data_idx]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\", line 106, in __getitem__\n",
            "    img = self.transform(img)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 67, in __call__\n",
            "    img = t(img)\n",
            "  File \"<ipython-input-38-03122ed3e5b5>\", line 59, in __getitem__\n",
            "    return self.data[data_idx]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\", line 106, in __getitem__\n",
            "    img = self.transform(img)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 67, in __call__\n",
            "    img = t(img)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
            "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 278, in normalize\n",
            "    if (std == 0).any():\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
            "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 27, in wrapped\n",
            "    return f(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-03122ed3e5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 278, in normalize\n",
            "    if (std == 0).any():\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}